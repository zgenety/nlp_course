{
 "cells": [
  {
   "cell_type": "raw",
   "source": [
    "Sorokin Evgeniy B21-DS-01\n",
    "Assignment2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implement context-sensitive spelling correction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Bigrams:\n",
    "    def __init__(self, main_path, additional_path):\n",
    "        # read files\n",
    "        self.gram_main = list(pd.read_csv(main_path).text)\n",
    "        self.gram_add = pd.read_csv(additional_path, header=None, sep=\";\")\n",
    "\n",
    "        # create hash tables of words and bigrams\n",
    "        self.word = {}\n",
    "        self.bigram = {}\n",
    "\n",
    "        # call function to generate hash tables\n",
    "        self.__transform_main()\n",
    "        self.__generate_dict()\n",
    "\n",
    "        # clear memory\n",
    "        self.gram_main = None\n",
    "        self.gram_add = None\n",
    "\n",
    "    def __transform_main(self) -> None:\n",
    "        \"\"\"This function count bigrams in main_file\"\"\"\n",
    "        self.gram_main = [b for l in self.gram_main for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
    "        self.gram_main = Counter(self.gram_main)\n",
    "\n",
    "    def __generate_dict(self) -> None:\n",
    "        \"\"\"Transform DFs to dictionary of words and bigrams\"\"\"\n",
    "        for i in tqdm(range(self.gram_add.shape[0])):\n",
    "            count = self.gram_add.iloc[i][0]\n",
    "            first = self.gram_add.iloc[i][1]\n",
    "            second = self.gram_add.iloc[i][2]\n",
    "\n",
    "            self.word[first] = self.word[first] + count if first in self.word else count\n",
    "            self.word[second] = self.word[second] + count if second in self.word else count\n",
    "            self.bigram[f\"{first} {second}\"] = count\n",
    "\n",
    "        for ws in tqdm(self.gram_main.keys()):\n",
    "            first, second = list(ws)\n",
    "            count = self.gram_main[ws]\n",
    "\n",
    "            self.word[first] = self.word[first] + count if first in self.word else count\n",
    "            self.word[second] = self.word[second] + count if second in self.word else count\n",
    "            self.bigram[f\"{first} {second}\"] = self.bigram[\n",
    "                                                   f\"{first} {second}\"] + count if f\"{first} {second}\" in self.bigram else count\n",
    "\n",
    "    def is_word_exist(self, word: str) -> bool:\n",
    "        return word in self.word\n",
    "\n",
    "    def count_bigram(self, word1: str, word2: str) -> int:\n",
    "        bi = word1 + \" \" + word2\n",
    "        return self.bigram[bi] if bi in self.bigram else 0\n",
    "\n",
    "    def count_word(self, w: str) -> int:\n",
    "        return self.word[w]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MoQeEsZvHvvi"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "class ContextSpellChecker:\n",
    "    def __init__(self, bigram: Bigrams, max_depth=2, lower_fr=100, trade_of = 100):\n",
    "        self.bgr = bigram\n",
    "        self.max_depth = max_depth\n",
    "        self.lower_fr = lower_fr\n",
    "        self.trade_of = trade_of\n",
    "\n",
    "    def edit(self, word):\n",
    "        \"\"\"Returns all variants of candidate words with a distance equal to 1, using Norvig part of code\"\"\"\n",
    "        letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        deletes = [L + R[1:] for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "        inserts = [L + c + R for L, R in splits for c in letters]\n",
    "        return list(set(deletes + transposes + replaces + inserts))\n",
    "\n",
    "    def merge_candidates(self, c1: list, c2: list):\n",
    "        \"\"\"Create all pairs of elements betwean two arrays\"\"\"\n",
    "        return [list(pair) for pair in list(itertools.product(c1, c2))]\n",
    "\n",
    "    def get_candidates(self, w: str, depth: int):\n",
    "        \"\"\"Create all candidates of givven word with distance n\"\"\"\n",
    "        can = [w]\n",
    "        for i in range(depth):\n",
    "            t1 = [self.edit(i) for i in can]\n",
    "            for c in t1: can += c\n",
    "            can = list(set(can))\n",
    "        return [w for w in list(set(can)) if self.bgr.is_word_exist(w)]\n",
    "\n",
    "    def get_info(self, s: str) -> tuple:\n",
    "        \"\"\"Split original string by spaces and create arrays of position correct and incorect words\"\"\"\n",
    "        splits = s.split()\n",
    "\n",
    "        # ad \"_\" as end/start of string\n",
    "        signature = [\"_\"]\n",
    "        for w in splits:\n",
    "            # create array like [\"-\", \"v\", \"x\", \"_\"] where v - correcct, x - with mistake\n",
    "            if self.bgr.is_word_exist(w) and self.bgr.count_word(w) > self.lower_fr:\n",
    "                signature.append(\"v\")\n",
    "            else:\n",
    "                signature.append(\"x\")\n",
    "        signature.append(\"_\")\n",
    "\n",
    "        splits = [\"_\"] + splits + [\"_\"]\n",
    "\n",
    "        return splits, signature\n",
    "\n",
    "    def best_count(self, w: str) -> str:\n",
    "        \"\"\"Return best condidate by freqyency of using\"\"\"\n",
    "        for d in range(1, self.max_depth + 1):\n",
    "\n",
    "            error = self.get_candidates(w, depth=d)\n",
    "            fin = [[i, self.bgr.count_word(i)] for i in error]\n",
    "\n",
    "            if fin != []:\n",
    "                break\n",
    "\n",
    "        return  [[max(fin, key=lambda x: x[1])[0]],\n",
    "                max(fin, key=lambda x: x[1])[1]/self.trade_of]\\\n",
    "            if fin != []  and max(fin, key=lambda x: x[1])[1] != 0 else\\\n",
    "            [[w], 1]\n",
    "\n",
    "    def best_previous(self, w: str, c: str) -> list:\n",
    "        \"return best word using correct next word or by frequency if bigram doesn't exist\"\n",
    "        for d in range(1, self.max_depth + 1):\n",
    "\n",
    "            error = self.get_candidates(w, depth=d)\n",
    "            can = self.merge_candidates(error, [c])\n",
    "            fin = [[i, self.bgr.count_bigram(*i)] for i in can]\n",
    "\n",
    "            if fin != [] and max(fin, key=lambda x: x[1])[1] != 0:\n",
    "                break\n",
    "        return max(fin, key=lambda x: x[1]) \\\n",
    "            if fin != [] and max(fin, key=lambda x: x[1])[1] != 0 else\\\n",
    "            self.best_count(w)\n",
    "\n",
    "    def best_next(self, c: str, w: str) -> list:\n",
    "        \"return best word using correct previous word or by frequency if bigram doesn't exist\"\n",
    "        for d in range(1, self.max_depth + 1):\n",
    "\n",
    "            error = self.get_candidates(w, depth=d)\n",
    "            can = self.merge_candidates([c], error)\n",
    "            fin = [[i, self.bgr.count_bigram(*i)] for i in can]\n",
    "\n",
    "            if fin != [] and max(fin, key=lambda x: x[1])[1] != 0:\n",
    "                break\n",
    "\n",
    "        return max(fin, key=lambda x: x[1]) \\\n",
    "            if fin != [] and max(fin, key=lambda x: x[1])[1] != 0 else \\\n",
    "            self.best_count(w)\n",
    "\n",
    "    def best_gram(self, w1: str, w2: str) -> list:\n",
    "        \"\"\"Search best fit ngramor or return best by frequency if bigram doesn't exist\"\"\"\n",
    "        for d in range(1, self.max_depth + 1):\n",
    "            error1 = self.get_candidates(w1, depth=d)\n",
    "            error2 = self.get_candidates(w2, depth=d)\n",
    "            can = self.merge_candidates(error1, error2)\n",
    "            fin = [[i, self.bgr.count_bigram(*i)] for i in can]\n",
    "\n",
    "            if fin != [] and max(fin, key=lambda x: x[1])[1] != 0:\n",
    "                break\n",
    "\n",
    "        return max(fin, key=lambda x: x[1]) \\\n",
    "            if fin != [] and max(fin, key=lambda x: x[1])[1] != 0 else \\\n",
    "            [[self.best_count(w1)[0][0], self.best_count(w2)[0][0]], 1]\n",
    "\n",
    "    def get_string(self, st: str) -> str:\n",
    "        s, sig = self.get_info(st)\n",
    "\n",
    "        # if all words in string are correct\n",
    "        if sig.count(\"x\") == 0:\n",
    "            return st\n",
    "\n",
    "        #looking all mistakes in string\n",
    "        while \"x\" in sig:\n",
    "            i = sig.index(\"x\")\n",
    "\n",
    "            #process different patern of the relative positions of words\n",
    "            match sig[i - 1] + sig[i] + sig[i + 1]:\n",
    "                case \"_x_\":\n",
    "                    s[i] = self.best_count(s[i])[0][0]\n",
    "                    sig[i] = \"v\"\n",
    "                case \"_xv\":\n",
    "                    s[i] = self.best_previous(s[i], s[i+1])[0][0]\n",
    "                    sig[i] = \"v\"\n",
    "                case \"vx_\":\n",
    "                    var = self.best_next(s[i-1], s[i])\n",
    "                    s[i] = self.best_next(s[i-1], s[i])[0][1] if len(var[0]) == 2 else self.best_next(s[i-1], s[i])[0][0]\n",
    "                    sig[i] = \"v\"\n",
    "                case \"vxv\":\n",
    "                    choose = [self.best_next(s[i-1], s[i]), self.best_previous(s[i], s[i+1])]\n",
    "                    best = max(choose, key=lambda x: x[1])\n",
    "                    s[i] = best[0][0] \\\n",
    "                        if len(best[0]) == 1 else \\\n",
    "                        best[0][choose.index(best) == 0]\n",
    "                    sig[i] = \"v\"\n",
    "                case \"_xx\":\n",
    "                    s[i], s[i+1] = self.best_gram(s[i], s[i+1])[0]\n",
    "                    sig[i], sig[i+1] = \"v\", \"v\"\n",
    "                case \"vxx\":\n",
    "                    choose = [self.best_next(s[i-1], s[i]), self.best_gram(s[i], s[i+1])]\n",
    "                    if choose[0][1] <= choose[1][1]:\n",
    "                        s[i], s[i+1] = choose[1][0]\n",
    "                        sig[i], sig[i+1] = \"v\", \"v\"\n",
    "                    else:\n",
    "                        s[i] = choose[0][0][0] if len(choose[0][0]) == 1 else choose[0][0][1]\n",
    "                        sig[i] = \"v\"\n",
    "        return \" \".join(s[1:-1:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oML-5sJwGRLE"
   },
   "source": [
    "## Justify your decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Xb_twOmVsC6"
   },
   "source": [
    "This solution is an improvement on Norvig's solution. This solution uses:\n",
    " - Dynamic depth of candidate search\n",
    "  - If a word is used more often than a certain amount, it is considered to exist and is not investigated for coreection.\n",
    "- To find a word in a suitable context, the Markov assumption is used and only the neighbors to the right and left of the word are considered, bigrams were used for this(Compiled from training data and files attached to the task).\n",
    "- Words and diagrams data structure is hash tables(dictionaries) since the search in them is carried out in O(1)\n",
    "- Work with all string, not only one word\n",
    "\n",
    "[Link for dataset](https://www.kaggle.com/datasets/samarthagarwal23/spelling-mistake-data-1mn/data?select=train.csv)\n",
    "\n",
    "Parameters:\n",
    "- max_depth - what are the max distance betwean original word and candidates\n",
    "- lower-fr - The boundary of how a part of a word is used in the data to consider it as existing.\n",
    "- trade_of - Coefficient which spelling or context to trust more (0; +inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46rk65S4GRSe"
   },
   "source": [
    "## Evaluate on a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1020385/1020385 [01:24<00:00, 12137.79it/s]\n",
      "100%|██████████| 1748336/1748336 [00:03<00:00, 513461.49it/s]\n"
     ]
    }
   ],
   "source": [
    "bigram = Bigrams(\"train.csv\", \"bigrams.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "csch = ContextSpellChecker(bigram, max_depth=2, lower_fr=100, trade_of=100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  text  \\\n0        project looks to mulesing genetic alternative   \n1    chemical agents used during protest at port au...   \n2    business chamber seeks budget infrastructure b...   \n3          3600 trips made to darwin tip after cyclone   \n4                     go between bridge to open july 5   \n..                                                 ...   \n295      council seeks job support for walkers workers   \n296             quarantine officers find bizarre items   \n297        support aired for indigenous language funds   \n298  deep fryer thought to have sparked manildra hotel   \n299             bypass construction gets frog friendly   \n\n                                        augmented_text  \n0        project looks to muelsnig ngeetic alternative  \n1    chemical agents used during LrotWst at port ah...  \n2    business hcmaber seeks budget infrastrcutuer b...  \n3          3600 trips made to adrwni tip after cyconle  \n4                     go net3een brisye to lprn july 5  \n..                                                 ...  \n295      council seeks job spupotr for walkrse wroekrs  \n296             quarQBtine obflcers find bizarre items  \n297        suppotr aired for indigenous language ufnsd  \n298  deep f3yeG thlughR to have sparked manildra hpteP  \n299             bypass construction g@tq fe9g friendly  \n\n[300 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>augmented_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>project looks to mulesing genetic alternative</td>\n      <td>project looks to muelsnig ngeetic alternative</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>chemical agents used during protest at port au...</td>\n      <td>chemical agents used during LrotWst at port ah...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>business chamber seeks budget infrastructure b...</td>\n      <td>business hcmaber seeks budget infrastrcutuer b...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3600 trips made to darwin tip after cyclone</td>\n      <td>3600 trips made to adrwni tip after cyconle</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>go between bridge to open july 5</td>\n      <td>go net3een brisye to lprn july 5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>295</th>\n      <td>council seeks job support for walkers workers</td>\n      <td>council seeks job spupotr for walkrse wroekrs</td>\n    </tr>\n    <tr>\n      <th>296</th>\n      <td>quarantine officers find bizarre items</td>\n      <td>quarQBtine obflcers find bizarre items</td>\n    </tr>\n    <tr>\n      <th>297</th>\n      <td>support aired for indigenous language funds</td>\n      <td>suppotr aired for indigenous language ufnsd</td>\n    </tr>\n    <tr>\n      <th>298</th>\n      <td>deep fryer thought to have sparked manildra hotel</td>\n      <td>deep f3yeG thlughR to have sparked manildra hpteP</td>\n    </tr>\n    <tr>\n      <th>299</th>\n      <td>bypass construction gets frog friendly</td>\n      <td>bypass construction g@tq fe9g friendly</td>\n    </tr>\n  </tbody>\n</table>\n<p>300 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_test_size = 300\n",
    "correct = 0\n",
    "test = pd.read_csv(\"test.csv\")[0:total_test_size]\n",
    "test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OwZWaX9VVs7B"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:32<00:00,  1.96it/s]\n"
     ]
    }
   ],
   "source": [
    "for row in tqdm(range(total_test_size)):\n",
    "    right = test.text[row]\n",
    "    original = test.augmented_text[row]\n",
    "    correct += csch.get_string(original) == right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 46.67%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy = {round((correct/total_test_size) * 100, 2)}%\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Norvig solution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('big.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())):\n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word):\n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word):\n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words):\n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word):\n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:42<00:00,  7.12it/s]\n"
     ]
    }
   ],
   "source": [
    "correct_n = 0\n",
    "for row in tqdm(range(total_test_size)):\n",
    "    right = test.text[row].split()\n",
    "    original = test.augmented_text[row].split()\n",
    "    t = []\n",
    "    for i in original:\n",
    "        t.append(correction(i))\n",
    "    correct_n += t==right"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 9.33%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy = {round((correct_n/total_test_size) * 100, 2)}%\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "These tests check whether the algorithms correct the entire string correctly. Tests have shown that a context-sensitive solution copes with the task much better. Norvig solution is faster in 3.85 times, but worse in 5 times."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
